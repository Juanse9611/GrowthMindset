{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2c52d5",
   "metadata": {},
   "source": [
    "# Análisis Financiero del Russell 1000 usando YahooQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33373efc",
   "metadata": {},
   "source": [
    "## Etapa 1: Obtener listado del Russell 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbf4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Company', 'Symbol', 'GICS Sector', 'GICS Sub-Industry'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_20360\\1279450636.py:16: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "# URL de la página\n",
    "url = 'https://en.wikipedia.org/wiki/Russell_1000_Index'\n",
    "\n",
    "# Encabezado para evitar el error 403\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "\n",
    "# Hacer la solicitud\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Leer las tablas HTML desde el contenido descargado\n",
    "tables = pd.read_html(response.text)\n",
    "\n",
    "# Inspeccionar la tabla 3 (asegúrate de que contenga los tickers)\n",
    "df_r1000 = tables[3]  # O cambia a otro índice si no es la tabla correcta\n",
    "\n",
    "# Mostrar las columnas disponibles\n",
    "print(df_r1000.columns)\n",
    "\n",
    "# Extraer los tickers si existe la columna 'Symbol'\n",
    "if 'Symbol' in df_r1000.columns:\n",
    "    tickers = df_r1000['Symbol'].tolist()\n",
    "    tickers=tickers[:100]\n",
    "else:\n",
    "    print(\"❌ La columna 'Symbol' no está en esta tabla.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748e327",
   "metadata": {},
   "source": [
    "## Etapa 2: Descargar EPS básico (últimos 4 años fiscales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b68df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahooquery import Ticker\n",
    "\n",
    "# Obtener datos financieros para todos los tickers de una vez\n",
    "t = Ticker(tickers)\n",
    "types=['BasicEPS','CashCashEquivalentsAndShortTermInvestments', 'EBITDA', 'TotalRevenue', 'InterestExpense', 'TaxProvision', 'ChangeInWorkingCapital','LongTermDebtAndCapitalLeaseObligation']\n",
    "eps_data = t.get_financial_data(types, trailing=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769d34c",
   "metadata": {},
   "source": [
    "## Etapa 3: Filtrar empresas con 4 años de EPS positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c964eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar empresas con valores negativos en variables críticas\n",
    "eps_data = eps_data[\n",
    "    (eps_data['EBITDA'] > 0) &\n",
    "    (eps_data['TotalRevenue'] > 0)\n",
    "]\n",
    "# Reemplazar InterestExpense nulos por 0 (caso típico)\n",
    "eps_data['InterestExpense'] = eps_data['InterestExpense'].fillna(0)\n",
    "eps_data['CashCashEquivalentsAndShortTermInvestments'] = eps_data['CashCashEquivalentsAndShortTermInvestments'].fillna(0)\n",
    "eps_data['TaxProvision'] = eps_data['TaxProvision'].fillna(0)\n",
    "eps_data['LongTermDebtAndCapitalLeaseObligation'] = eps_data['LongTermDebtAndCapitalLeaseObligation'].fillna(0)\n",
    "\n",
    "#Margin Ebitda\n",
    "eps_data['EBITDA_Margin'] = eps_data['EBITDA'] / eps_data['TotalRevenue']\n",
    "#Límite 1\n",
    "Den = eps_data['EBITDA'] - eps_data['TaxProvision'] - eps_data['ChangeInWorkingCapital']\n",
    "eps_data['Límite1'] = eps_data['InterestExpense'] / Den\n",
    "\n",
    "#Límite 2\n",
    "num = eps_data['LongTermDebtAndCapitalLeaseObligation'] - eps_data['CashCashEquivalentsAndShortTermInvestments']\n",
    "Den2 = eps_data['EBITDA']\n",
    "eps_data['Límite2'] = num / Den\n",
    "\n",
    "# Mantener solo tickers con al menos 4 registros de EPS\n",
    "ticker_counts = eps_data.index.get_level_values(0).value_counts()\n",
    "tickers_validos = ticker_counts[ticker_counts >= 4].index.tolist()\n",
    "\n",
    "# Filtrar dataset para dejar solo esas empresas\n",
    "eps_filtrado = eps_data.loc[tickers_validos]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1fce95",
   "metadata": {},
   "source": [
    "## Etapa 4: Reorganizar el EPS en un DataFrame limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8681636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetear índice y limpiar columnas\n",
    "eps_filtrado = eps_filtrado.reset_index()\n",
    "eps_filtrado = eps_filtrado[['symbol', 'asOfDate', 'BasicEPS', 'EBITDA_Margin', 'Límite1','Límite2']]\n",
    "eps_filtrado['asOfDate'] = pd.to_datetime(eps_filtrado['asOfDate'])\n",
    "\n",
    "# Agregar filas con +3 días para cubrir posibles feriados/no cotización\n",
    "import datetime\n",
    "\n",
    "rows_extra = []\n",
    "for idx, row in eps_filtrado.iterrows():\n",
    "    new_date = row['asOfDate'] + datetime.timedelta(days=3)\n",
    "    rows_extra.append({'symbol': row['symbol'], 'asOfDate': new_date, 'BasicEPS': row['BasicEPS'],'EBITDA_Margin': row['EBITDA_Margin'],'Límite1': row['Límite1'],'Límite2': row['Límite2']})\n",
    "\n",
    "eps_fiscal = pd.concat([eps_filtrado, pd.DataFrame(rows_extra)], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcc27c",
   "metadata": {},
   "source": [
    "## Etapa 5: Descargar precios históricos (últimos 5 años)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188e3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos actualizados hasta 2025-11-14, no se descarga nada.\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "\n",
    "START_DATE = \"2020-05-30\"          # Fecha inicial fija para descargar históricos\n",
    "WEEKS_BACK = 2                     # Cuántas semanas antes de hoy descargar\n",
    "PARQUET_FILE = \"prices.parquet\"    # Archivo local donde se guardan los datos\n",
    "\n",
    "\n",
    "# CALCULAR FECHA FINAL (end_date)\n",
    "# Dos semanas antes de la fecha actual\n",
    "# .normalize() asegura que no tenga hora, para evitar problemas al comparar días\n",
    "\n",
    "today = pd.Timestamp.today().normalize()\n",
    "end_date = today - timedelta(days=WEEKS_BACK * 7)\n",
    "\n",
    "# Ajustar end_date al último día hábil (lunes-viernes)\n",
    "while end_date.weekday() > 4:\n",
    "    end_date -= timedelta(days=1)\n",
    "\n",
    "# SI EL ARCHIVO YA EXISTE, CARGARLO Y VERIFICAR SI NECESITA ACTUALIZACIÓN\n",
    "\n",
    "if os.path.exists(PARQUET_FILE):\n",
    "\n",
    "    # Cargar datos existentes\n",
    "    prices = pd.read_parquet(PARQUET_FILE)\n",
    "\n",
    "    # Última fecha disponible en el archivo local (sin hora)\n",
    "    last_date = prices[\"asOfDate\"].max().normalize()\n",
    "\n",
    "    # Si el archivo ya está actualizado, no se descarga nada\n",
    "    if last_date >= end_date:\n",
    "        print(f\"Datos actualizados hasta {last_date.date()}, no se descarga nada.\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(f\"Actualizando datos desde {last_date.date()} hasta {end_date.date()}...\")\n",
    "\n",
    "        # Lista de tickers válidos ya almacenados\n",
    "        tickers_validos = prices[\"symbol\"].unique().tolist()\n",
    "\n",
    "        # Crear objeto Ticker\n",
    "        t = Ticker(tickers_validos)\n",
    "\n",
    "        # Descargar solo la parte faltante\n",
    "        new_data = t.history(\n",
    "            start=last_date + timedelta(days=1),  # día siguiente a los datos existentes\n",
    "            end=end_date,\n",
    "            interval=\"1d\"\n",
    "        )\n",
    "\n",
    "        # Limpiar columnas y formatear\n",
    "        new_data = (\n",
    "            new_data\n",
    "            .drop(columns=['open', 'high', 'low', 'volume', 'adjclose', 'dividends', 'splits'])\n",
    "            .reset_index()[['symbol', 'date', 'close']]\n",
    "            .rename(columns={'date': 'asOfDate'})\n",
    "        )\n",
    "\n",
    "        # Normalizar fecha (sin hora)\n",
    "        new_data['asOfDate'] = pd.to_datetime(new_data['asOfDate']).dt.normalize()\n",
    "\n",
    "        # Combinar histórico viejo + nuevo, eliminando duplicados\n",
    "        prices = pd.concat([prices, new_data], ignore_index=True).drop_duplicates()\n",
    "\n",
    "        # Guardar archivo actualizado\n",
    "        prices.to_parquet(PARQUET_FILE, index=False)\n",
    "\n",
    "else:\n",
    "  \n",
    "    # PRIMERA DESCARGA (si no existe el archivo)\n",
    "\n",
    "    print(\"Descargando datos históricos por primera vez...\")\n",
    "\n",
    "    # t_valid ya debería existir antes de esta etapa con los tickers válidos finales\n",
    "    t = Ticker(tickers_validos)\n",
    "\n",
    "    # Descargar histórico completo una sola vez\n",
    "    prices = t.history(start=START_DATE, end=end_date, interval=\"1d\")\n",
    "\n",
    "    # Limpiar columnas\n",
    "    prices = (\n",
    "        prices\n",
    "        .drop(columns=['open', 'high', 'low', 'volume', 'adjclose', 'dividends', 'splits'])\n",
    "        .reset_index()[['symbol', 'date', 'close']]\n",
    "        .rename(columns={'date': 'asOfDate'})\n",
    "    )\n",
    "\n",
    "    # Normalizar fecha (sin hora)\n",
    "    prices['asOfDate'] = pd.to_datetime(prices['asOfDate']).dt.normalize()\n",
    "\n",
    "    # Guardar archivo\n",
    "    prices.to_parquet(PARQUET_FILE, index=False)\n",
    "\n",
    "print(\"Proceso completado.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98664731",
   "metadata": {},
   "source": [
    "## Etapa 6: Unir EPS con precios para calcular PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d4ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge con símbolo y fecha\n",
    "df_merge = pd.merge(prices, eps_fiscal, on=['symbol', 'asOfDate'])\n",
    "\n",
    "# Filtrar empresas con 4 años válidos\n",
    "valid_counts = df_merge['symbol'].value_counts()\n",
    "df_merge = df_merge[df_merge['symbol'].isin(valid_counts[valid_counts >= 4].index)]\n",
    "\n",
    "# Calcular PER\n",
    "df_merge['PER'] = df_merge['close'] / df_merge['BasicEPS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d421fd",
   "metadata": {},
   "source": [
    "## Etapa 7: Calcular promedio EPS, PER y precio objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f972f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resumen = df_merge.sort_values(['symbol', 'asOfDate'])\n",
    "\n",
    "promedios = (\n",
    "    Resumen.groupby('symbol')\n",
    "    .agg(PER_promedio=('PER', 'mean'),\n",
    "         EPS_promedio=('BasicEPS', 'mean'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calcular precio objetivo con PER promedio y EPS promedio\n",
    "promedios['Precio_Objetivo'] = promedios['EPS_promedio'] * promedios['PER_promedio']\n",
    "promedios['Precio_Objetivo_85'] = promedios['Precio_Objetivo'] * 0.85\n",
    "\n",
    "# Filtrar empresas con PER promedio < 40\n",
    "promedios_filtrados = promedios[promedios['PER_promedio'] < 40]\n",
    "\n",
    "# Obtener lista de símbolos válidos\n",
    "tickers_filtrados = promedios_filtrados['symbol'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d8590",
   "metadata": {},
   "source": [
    "## Etapa 8: Obtener precios actuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2491adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "t_validf = Ticker(tickers_filtrados)\n",
    "precios_actuales = t_validf.history(start='2025-09-24', end='2025-09-25', interval='1d')\n",
    "precios_actuales = precios_actuales.reset_index()[['symbol', 'close']]\n",
    "precios_actuales = precios_actuales.groupby('symbol').last().reset_index()\n",
    "precios_actuales.rename(columns={'close': 'Precio_Actual'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16f228",
   "metadata": {},
   "source": [
    "## Etapa 9: Resultado final y exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "137b7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo guardado como 'Russell_1000_Valoraciones.xlsx'\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.merge(promedios_filtrados, precios_actuales, on='symbol', how='inner')\n",
    "\n",
    "# Reordenar columnas y guardar\n",
    "df_final = df_final[['symbol', 'Precio_Actual', 'Precio_Objetivo_85', 'PER_promedio', 'EPS_promedio']]\n",
    "df_final.columns = ['Empresa', 'Precio Actual', 'Precio Objetivo', 'PER Promedio', 'EPS Promedio']\n",
    "\n",
    "# Exportar\n",
    "df_final.to_excel('Russell_1000_Valoraciones.xlsx', index=False)\n",
    "print(\"Archivo guardado como 'Russell_1000_Valoraciones.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a807efd",
   "metadata": {},
   "source": [
    "# Etapa 10 – Evolución anual del Margen EBITDA por empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96334f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado 'Margen EBITDA por Año' con empresas que tienen PER Promedio < 40.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Filtrar eps_fiscal con los tickers que tienen PER promedio < 40\n",
    "eps_filtrado_final = eps_fiscal[eps_fiscal['symbol'].isin(tickers_filtrados)].copy()\n",
    "\n",
    "# Agregar columna de año\n",
    "eps_filtrado_final['Año'] = eps_filtrado_final['asOfDate'].dt.year\n",
    "\n",
    "# Calcular promedios anuales\n",
    "margen_anual = eps_filtrado_final.groupby(['symbol', 'Año'])[['EBITDA_Margin', 'Límite1']].mean().reset_index()\n",
    "margen_anual.rename(columns={\n",
    "    'symbol': 'Empresa',\n",
    "    'EBITDA_Margin': 'Margen EBITDA',\n",
    "    'Límite1': 'Límite1'\n",
    "}, inplace=True)\n",
    "\n",
    "# Exportar a nueva hoja en el Excel existente\n",
    "from openpyxl import load_workbook\n",
    "with pd.ExcelWriter('Russell_1000_Valoraciones.xlsx', mode='a', engine='openpyxl') as writer:\n",
    "    margen_anual.to_excel(writer, sheet_name='Margen EBITDA por Año', index=False)\n",
    "\n",
    "print(\"Guardado 'Margen EBITDA por Año' con empresas que tienen PER Promedio < 40.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
